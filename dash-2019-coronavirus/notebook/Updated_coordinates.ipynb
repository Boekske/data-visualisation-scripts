{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T11:14:47.053652Z",
     "start_time": "2020-02-21T11:14:47.050939Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import math \n",
    "import os\n",
    "\n",
    "from opencage.geocoder import OpenCageGeocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import xlsx file and store each sheet in to a df list\n",
    "xl_file = pd.ExcelFile('./data.xls',)\n",
    "\n",
    "dfs = {sheet_name: xl_file.parse(sheet_name) \n",
    "          for sheet_name in xl_file.sheet_names}\n",
    "\n",
    "# Data from each sheet can be accessed via key\n",
    "keyList = list(dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each sheet as csv to improve performance (these csv files will be used for app.py)\n",
    "for key, df in dfs.items():\n",
    "    dfs[key].to_csv('./raw_data/{}.csv'.format(key), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T11:14:52.459673Z",
     "start_time": "2020-02-21T11:14:48.641735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data cleansing\n",
    "for key, df in dfs.items():\n",
    "    dfs[key].loc[:,'Confirmed'].fillna(value=0, inplace=True)\n",
    "    dfs[key].loc[:,'Deaths'].fillna(value=0, inplace=True)\n",
    "    dfs[key].loc[:,'Recovered'].fillna(value=0, inplace=True)\n",
    "    dfs[key]=dfs[key].astype({'Confirmed':'int64', 'Deaths':'int64', 'Recovered':'int64'})\n",
    "    # Change as China for coordinate search\n",
    "    dfs[key]=dfs[key].replace({'Country/Region':'Mainland China'}, 'China')\n",
    "    # Add a zero to the date so can be convert by datetime.strptime as 0-padded date\n",
    "    dfs[key]['Last Update'] = '0' + dfs[key]['Last Update']\n",
    "    # Convert time as Australian eastern daylight time\n",
    "    dfs[key]['Date_last_updated_AEDT'] = [datetime.strptime(d, '%m/%d/%Y %H:%M') for d in dfs[key]['Last Update']]\n",
    "    dfs[key]['Date_last_updated_AEDT'] = dfs[key]['Date_last_updated_AEDT'] + timedelta(hours=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code of the following cell is for generating cumulative data for lineplot. They should run when updating heroku server folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative data of each region are generated!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./cumulative_data'):\n",
    "    os.makedirs('./cumulative_data')\n",
    "\n",
    "# As countries (China, US, Canada and Australia) have duplicates as these counties have Province/State data\n",
    "for Region in set(dfs[keyList[0]]['Country/Region']): \n",
    "    # Function for generating cumulative line plot for each Country/Region\n",
    "    CaseType = ['Confirmed', 'Recovered', 'Deaths']\n",
    "\n",
    "    # Construct confirmed cases dataframe for line plot\n",
    "    df_region = pd.DataFrame(columns=['Confirmed', 'Recovered', 'Deaths', 'Date_last_updated_AEDT'])\n",
    "\n",
    "    for key, df in dfs.items():\n",
    "        # As Country name will not be in the dataframe when there is no cases\n",
    "        if Region in list(df['Country/Region']):\n",
    "            dfTpm = df.groupby(['Country/Region']).agg({'Confirmed':np.sum, \n",
    "                                                        'Recovered':np.sum, \n",
    "                                                        'Deaths':np.sum, \n",
    "                                                        'Date_last_updated_AEDT':'first'})\n",
    "            df_region = df_region.append(dfTpm.loc[Region, :])        \n",
    "        else:\n",
    "            dfTpm2 = pd.DataFrame({'Confirmed':[0],\n",
    "                                   'Recovered':[0],\n",
    "                                   'Deaths':[0],\n",
    "                                   'Date_last_updated_AEDT':[df['Date_last_updated_AEDT'][0]]}, index=[Region])\n",
    "            df_region = df_region.append(dfTpm2)\n",
    "\n",
    "    # Select the latest data from a given date\n",
    "    df_region['date_day']=[d.date() for d in df_region['Date_last_updated_AEDT']]\n",
    "    df_region=df_region.groupby(by=df_region['date_day'], sort=False).first()\n",
    "\n",
    "    df_region=df_region.reset_index(drop=True)\n",
    "    df_region['New'] = -(df_region['Confirmed'].shift(-1) - df_region['Confirmed'])\n",
    "    df_region.at[df_region.shape[0]-1, 'New'] = 0\n",
    "    df_region = df_region.astype({'New':int})\n",
    "    df_region.to_csv('./cumulative_data/{}.csv'.format(Region), index = False)\n",
    "\n",
    "print('Cumulative data of each region are generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part is for saving all coordinates as my own database. By doing so, `opencage.geocoder` does not need to go through all regions everytime (as most regions are already have coordinates in this database). Only new added regions will be called for coordinates via `opencage.geocoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinateCalling(queryData: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Using opencage.geocoder to call coordinates for these regions\n",
    "    Add coordinates for each region in the list for the latest table sheet\n",
    "    '''\n",
    "    key = 'b33700b33d0a446aa6e16c0b57fc82d1'  # get api key from:  https://opencagedata.com\n",
    "    geocoder = OpenCageGeocode(key)\n",
    "\n",
    "    list_lat = []   # create empty lists\n",
    "    list_long = []  \n",
    "\n",
    "    for index, row in queryData.iterrows(): # iterate over rows in dataframe\n",
    "\n",
    "        City = row['Province/State']\n",
    "        State = row['Country/Region']\n",
    "\n",
    "        # Note that 'nan' is float\n",
    "        if type(City) is str:\n",
    "            query = str(City)+','+str(State)\n",
    "            results = geocoder.geocode(query)   \n",
    "            lat = results[0]['geometry']['lat']\n",
    "            long = results[0]['geometry']['lng']\n",
    "\n",
    "            list_lat.append(lat)\n",
    "            list_long.append(long)\n",
    "        else:\n",
    "            query = str(State)\n",
    "            results = geocoder.geocode(query)   \n",
    "            lat = results[0]['geometry']['lat']\n",
    "            long = results[0]['geometry']['lng']\n",
    "\n",
    "            list_lat.append(lat)\n",
    "            list_long.append(long)\n",
    "\n",
    "    # create new columns from lists    \n",
    "    queryData['lat'] = list_lat   \n",
    "    queryData['lon'] = list_long\n",
    "    \n",
    "    return queryData   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no added region. Data has been saved\n"
     ]
    }
   ],
   "source": [
    "# Import coordinate database\n",
    "GeoDB = pd.read_csv('./coordinatesDB.csv')\n",
    "\n",
    "# Save the latest data into targetData\n",
    "targetData = dfs[keyList[0]]\n",
    "\n",
    "# Assign coordinates to regions from coordinates database\n",
    "resultData = pd.merge(targetData, GeoDB, how='left', on=['Province/State', 'Country/Region'])\n",
    "\n",
    "# Find regions do not have coordinates\n",
    "queryData = resultData.loc[resultData['lat'].isnull()]\n",
    "queryData = queryData[['Province/State', 'Country/Region']]\n",
    "\n",
    "# There will be two possible results\n",
    "# One queryData is empty when all regions already have coordinates from coordinate database,\n",
    "# and queryData is not empty when there are new added region names\n",
    "if queryData.shape[0] != 0:\n",
    "    coordinateCalling(queryData)\n",
    "    # Update database by adding the new coordinates into it\n",
    "    GeoDB = pd.concat([GeoDB, queryData], ignore_index=True)\n",
    "    # Save the coordinates database\n",
    "    GeoDB.to_csv('./coordinatesDB.csv', index = False)\n",
    "    # Assign coordinates to all regions using the latest coordinates database\n",
    "    finalData = pd.merge(targetData, GeoDB, how='left', on=['Province/State', 'Country/Region'] )\n",
    "    \n",
    "    # To check if there is still regions without coordinates (There should not be)\n",
    "    if finalData.loc[finalData['lat'].isnull()].shape[0] == 0:\n",
    "        # Save the data for heroku app\n",
    "        finalData.to_csv('./{}_data.csv'.format(keyList[0]), index = False)\n",
    "        print('New regions now have corordinates. Data has been saved.')\n",
    "    else:\n",
    "        # There might be some issues\n",
    "        print('Please check your data') \n",
    "else:\n",
    "    # Assign coordinates to all regions using the latest coordinates database\n",
    "    finalData = pd.merge(targetData, GeoDB, how='left', on=['Province/State', 'Country/Region'] )\n",
    "    # Save the data for heroku app\n",
    "    finalData.to_csv('./{}_data.csv'.format(keyList[0]), index = False)\n",
    "    print('There is no added region. Data has been saved')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Province/State, Country/Region]\n",
       "Index: []"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all csv files and copy to heroku folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A variable for using in bash \n",
    "# Refer to https://stackoverflow.com/questions/19579546/can-i-access-python-variables-within-a-bash-or-script-ipython-notebook-c\n",
    "fileName1 = keyList[0]\n",
    "fileName2 = keyList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been transferred to heroku folder.\n",
      "You are now good to update heroku app!\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$fileName1\" \"$fileName2\"\n",
    "# Delete previous data from heroku folder\n",
    "rm ../../heroku_app/dash_coronavirus_2019/$2_data.csv\n",
    "# Delete previous data from current folder\n",
    "rm ./$2_data.csv\n",
    "# Copy all required data to heroku folder\n",
    "cp ./data.xls ../../heroku_app/dash_coronavirus_2019/\n",
    "cp ./raw_data/*.csv ../../heroku_app/dash_coronavirus_2019/raw_data/\n",
    "cp ./cumulative_data/*.csv ../../heroku_app/dash_coronavirus_2019/cumulative_data/\n",
    "cp ./$1_data.csv ../../heroku_app/dash_coronavirus_2019/\n",
    "echo \"All files have been transferred to heroku folder.\"\n",
    "echo \"You are now good to update heroku app!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
